{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34895a73-81bd-41c4-98e9-8a370c41c678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cabac3104e4a27a2e794a2dee47453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./merged_model/tokenizer_config.json',\n",
       " './merged_model/special_tokens_map.json',\n",
       " './merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##合并lora和基础权重,合并完成之后不用再次运行这段代码！！！\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B\"  # 替换为你的模型路径\n",
    "lora_path = \"./DeepSeek-R1-Distill-Qwen-7B_LoRA\"  # 替换为你的 LoRA 适配器路径\n",
    "\n",
    "# 加载基础模型和 LoRA 适配器\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # 加载 tokenizer\n",
    "lora_model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "\n",
    "# 合并 LoRA 适配器到基础模型\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# 保存合并后的模型和 tokenizer\n",
    "merged_model.save_pretrained(\"./merged_model\")  # 保存合并后的模型\n",
    "tokenizer.save_pretrained(\"./merged_model\")  # 保存 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc8125-c3ac-4eee-bbc5-cf46b04df9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 23:55:13 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 02-19 23:55:14 config.py:2382] Downcasting torch.float32 to torch.float16.\n",
      "INFO 02-19 23:55:20 config.py:542] This model supports multiple tasks: {'classify', 'score', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 02-19 23:55:20 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-19 23:55:20 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 02-19 23:55:20 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='./merged_model', speculative_config=None, tokenizer='./merged_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=./merged_model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-19 23:55:21 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-19 23:55:21 model_runner.py:1110] Starting to load model ./merged_model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cdadc62ccc4f65b98fc6466eba5342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 23:55:26 model_runner.py:1115] Loading model weights took 14.2717 GB\n",
      "INFO 02-19 23:55:27 worker.py:267] Memory profiling takes 0.89 seconds\n",
      "INFO 02-19 23:55:27 worker.py:267] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.90) = 42.78GiB\n",
      "INFO 02-19 23:55:27 worker.py:267] model weights take 14.27GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 27.05GiB.\n",
      "INFO 02-19 23:55:27 executor_base.py:110] # CUDA blocks: 31659, # CPU blocks: 4681\n",
      "INFO 02-19 23:55:27 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 3.86x\n",
      "INFO 02-19 23:55:29 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:13<00:00,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 23:55:43 model_runner.py:1562] Graph capturing finished in 13 secs, took 0.80 GiB\n",
      "INFO 02-19 23:55:43 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 16.51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载成功！\n",
      "模型已加载，现在可以开始提问了！输入 'exit' 退出。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "请输入问题:  多谢这位公子还有两位姑娘救了我们大人。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s, est. speed input: 53.96 toks/s, output: 36.91 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: 这就是宫里的规矩，你快去吧。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "请输入问题:  你倒为我打算得清楚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s, est. speed input: 86.58 toks/s, output: 38.25 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: 好，我这就去，还不快去快去，别耽误了。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "请输入问题:  谢了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.46it/s, est. speed input: 312.84 toks/s, output: 35.75 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: 不谢，快去吧。\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "# 初始化模型\n",
    "def load_model(model_name):\n",
    "    try:\n",
    "        model = vllm.LLM(model_name)\n",
    "        print(\"模型加载成功！\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"加载模型失败: {e}\")\n",
    "        return None\n",
    "\n",
    "# 初始化对话历史\n",
    "def initialize_history():\n",
    "    return []\n",
    "\n",
    "# 格式化对话历史\n",
    "def format_history(history):\n",
    "    formatted_history = \"\\n\".join([f\"User: {item}\" if i % 2 == 0 else f\"Assistant: {item}\" for i, item in enumerate(history)])\n",
    "    return formatted_history\n",
    "\n",
    "# 生成完整的提示，包含历史对话和当前用户输入\n",
    "def generate_prompt(prompt, history):\n",
    "    formatted_history = format_history(history)\n",
    "    full_prompt = f\"{formatted_history}\\n\\nUser: {prompt}\\n\\nAssistant:\"\n",
    "    return full_prompt\n",
    "\n",
    "# 生成回答并更新历史记录\n",
    "def generate_response(model, prompt, history):\n",
    "    try:\n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=1280,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop=[\"\\n\\nUser:\", \"\\n\\nAssistant:\"]  # 使用 `stop` 参数避免生成多余内容\n",
    "        )\n",
    "        output = model.generate([prompt], sampling_params=sampling_params)\n",
    "        raw_response = output[0].outputs[0].text.strip()\n",
    "        return raw_response\n",
    "    except Exception as e:\n",
    "        print(f\"生成回答失败: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 主程序\n",
    "model_name = \"./merged_model\"  # 替换为合并后的模型路径\n",
    "model = load_model(model_name)\n",
    "if model is None:\n",
    "    print(\"模型加载失败，检查模型路径是否正确\")\n",
    "else:\n",
    "    dialogue_history = initialize_history()\n",
    "    print(\"模型已加载，现在可以开始提问了！输入 'exit' 退出。\")\n",
    "    while True:\n",
    "        # 打印对话历史\n",
    "        # print(\"=======对话历史=====\")\n",
    "        # print(format_history(dialogue_history))\n",
    "        # print(\"=========\")\n",
    "        \n",
    "        user_input = input(\"\\n请输入问题: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"退出程序。\")\n",
    "            break\n",
    "        \n",
    "        # 生成完整的提示\n",
    "        prompt = generate_prompt(user_input, dialogue_history)\n",
    "        \n",
    "        # 生成回答\n",
    "        response = generate_response(model, prompt, dialogue_history)\n",
    "        if response is not None:\n",
    "            print(f\"\\nAssistant: {response}\")\n",
    "            # 更新对话历史\n",
    "            dialogue_history.append(user_input)\n",
    "            dialogue_history.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12095170-dddb-4584-8f57-8ecaf01ef85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
